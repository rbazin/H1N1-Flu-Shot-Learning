{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep neural network optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports\n",
    "========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions de traitement des donnees\n",
    "def numerical_impute(data, numerical_list):\n",
    "    imputer_numerical = SimpleImputer(\n",
    "        strategy='constant', fill_value=-1, missing_values=np.nan)\n",
    "    data_numerical = data.loc[:, numerical_list]\n",
    "    data_numerical_imputed = imputer_numerical.fit_transform(data_numerical)\n",
    "    data_numerical_imputed = pd.DataFrame(\n",
    "        data_numerical_imputed, columns=numerical_list)\n",
    "    return data_numerical_imputed\n",
    "\n",
    "\n",
    "def categorical_imputing(data, categorical_list):\n",
    "    # Imputing\n",
    "    imputer_categorical = SimpleImputer(\n",
    "        strategy='constant', fill_value='missing', missing_values=np.nan)\n",
    "    data_categorical = data.loc[:, categorical_list]\n",
    "    data_categorical = imputer_categorical.fit_transform(data_categorical)\n",
    "    data_categorical_imputed = pd.DataFrame(\n",
    "        data_categorical, columns=categorical_list)\n",
    "    return data_categorical_imputed\n",
    "\n",
    "\n",
    "def categorical_impute_one_hot(data, categorical_list):\n",
    "    # Imputing\n",
    "    data_categorical_imputed = categorical_imputing(data, categorical_list)\n",
    "\n",
    "    # One hot encoding\n",
    "    data_one_hot = pd.get_dummies(data_categorical_imputed)\n",
    "\n",
    "    return data_one_hot\n",
    "\n",
    "\n",
    "def data_clean(data, numerical_list, categorical_list):\n",
    "    # Changer les listes de features et les fonctions correspondantes\n",
    "    data_categorical_encoded = categorical_impute_one_hot(\n",
    "        data, categorical_list)\n",
    "    data_numerical_imputed = numerical_impute(data, numerical_list)\n",
    "    data_imputed_encoded = pd.merge(\n",
    "        data_numerical_imputed, data_categorical_encoded, left_index=True, right_index=True)\n",
    "\n",
    "    return data_imputed_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data prep\n",
    "========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path des données\n",
    "LABELS_TRAINING_PATH = os.path.join(\"data\", \"training_set_labels.csv\")\n",
    "FEATURES_TRAINING_PATH = os.path.join(\"data\", \"training_set_features.csv\")\n",
    "\n",
    "# On charge les données\n",
    "features = pd.read_csv(FEATURES_TRAINING_PATH, sep=\",\", header=0)\n",
    "labels = pd.read_csv(LABELS_TRAINING_PATH, sep=\",\", header=0)\n",
    "data = pd.merge(features, labels, on=\"respondent_id\")\n",
    "respondent_id = data['respondent_id']\n",
    "\n",
    "# Listes de features complètes\n",
    "arg_list = list(data.keys())\n",
    "features_list = arg_list.copy()\n",
    "features_list.remove(\"h1n1_vaccine\")\n",
    "features_list.remove(\"seasonal_vaccine\")\n",
    "features_list.remove(\"respondent_id\")\n",
    "\n",
    "# Différentes listes de features utiles\n",
    "labels_list = ['h1n1_vaccine', 'seasonal_vaccine']\n",
    "categorical_list = ['age_group', 'education', 'race', 'sex', 'income_poverty', 'marital_status', 'rent_or_own',\n",
    "                    'employment_status', 'hhs_geo_region', 'census_msa', 'employment_industry', 'employment_occupation']\n",
    "categorical_list_one_hot = ['race', 'sex', 'marital_status', 'rent_or_own', 'employment_status',\n",
    "                            'hhs_geo_region', 'census_msa', 'employment_industry', 'employment_occupation']\n",
    "categorical_list_ordinal = [\n",
    "    k for k in categorical_list if k not in categorical_list_one_hot]\n",
    "numerical_list = [k for k in features_list if k not in categorical_list]\n",
    "\n",
    "#\n",
    "labels.drop(\"respondent_id\", inplace=True, axis=1)\n",
    "Y = labels.to_numpy()\n",
    "X = data_clean(data, numerical_list, categorical_list).to_numpy()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On scale les données\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train_scaled, X_valid_scaled, Y_train, Y_valid = train_test_split(X_train_scaled, Y_train, test_size=0.3, random_state=1)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_network(drops, nbr_layers, lrs, nbr_neurons, X_train_scaled, Y_train, X_test_scaled, Y_test, X_valid_scaled, Y_valid):\n",
    "    score = 0\n",
    "    best_params = []\n",
    "    my_callbacks = []\n",
    "    my_callbacks.append(keras.callbacks.ModelCheckpoint(\"checkpoint_model.h5\", save_best_only=True))\n",
    "    my_callbacks.append(keras.callbacks.EarlyStopping(patience=10))\n",
    "    \n",
    "    for drop in drops:\n",
    "        for nbr_layer in nbr_layers:\n",
    "            for lr in lrs:\n",
    "                for nbr_neuron in nbr_neurons:\n",
    "                    # We build the model with the hyperparameters\n",
    "                    model = Sequential()    \n",
    "                    model.add(Input(shape=X_train_scaled[0].shape))\n",
    "\n",
    "                    for i in range(nbr_layer):\n",
    "                        model.add(Dense(nbr_neuron, activation=\"relu\"))\n",
    "                        model.add(Dropout(drop))\n",
    "\n",
    "                    model.add(Dense(2, activation=\"sigmoid\"))\n",
    "                    optimizer = keras.optimizers.Adam()\n",
    "                    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=['AUC'])\n",
    "                    model.fit(X_train_scaled, Y_train, epochs=500, validation_data=(X_valid_scaled, Y_valid), batch_size=64, callbacks=my_callbacks)\n",
    "                    pred = model.predict(X_test_scaled)\n",
    "\n",
    "                    if score < roc_auc_score(Y_test, pred):\n",
    "                        score = roc_auc_score(Y_test, pred)\n",
    "                        best_params = [drop, nbr_layer, lr, nbr_neuron]\n",
    "    \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = 0.25\n",
    "nbr_layers = 4\n",
    "lr = 1\n",
    "nbr_neurons = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()    \n",
    "model.add(Input(shape=X_train_scaled[0].shape))\n",
    "for i in range(nbr_layers):\n",
    "    model.add(Dense(nbr_neurons, activation=\"relu\"))\n",
    "    model.add(Dropout(drop))\n",
    "model.add(Dense(2, activation=\"sigmoid\"))\n",
    "optimizer = keras.optimizers.Adam()\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=['AUC'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callbacks = []\n",
    "my_callbacks.append(keras.callbacks.ModelCheckpoint(\"checkpoint_model.h5\", save_best_only=True))\n",
    "my_callbacks.append(keras.callbacks.EarlyStopping(patience=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_scaled, Y_train, epochs=500, validation_data=(X_valid_scaled, Y_valid), batch_size=64, callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.839896832853844\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test_scaled)\n",
    "print(roc_auc_score(Y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = [0.1, 0.2, 0.3, 0.5]\n",
    "nbr_layers = [2, 5, 20, 100]\n",
    "lrs = [1e-2, 1e-1, 0.5, 1]\n",
    "nbr_neurons = [10, 25, 50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = optimize_network(drops, nbr_layers, lrs, nbr_neurons, X_train_scaled, Y_train, X_test_scaled, Y_test, X_valid_scaled, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best_params, \"DNN_best_params.save\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88b22dca2824d609b93780823578fec25c709e92ed87a3b4977ff37edccce9e0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
